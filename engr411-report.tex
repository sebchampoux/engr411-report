\documentclass[12pt,a4paper,titlepage]{article}
\linespread{2.0}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}

\begin{document}

\title{Technical Report on Convolutional Neural Networks}
\author{Sebastien Champoux
\\ 40133449
\\ Concordia University
\\ ENGR 411 Technical Report
}
\maketitle

\begin{abstract}
I'm baby semiotics salvia literally +1 af coloring book woke banh mi gentrify organic wolf 8-bit tote bag chartreuse chicharrones. Helvetica cardigan next level art party kickstarter, vexillologist banh mi master cleanse actually street art tumeric poke whatever.
\end{abstract}
\clearpage

\tableofcontents
\clearpage

\listoffigures
\clearpage

\section{Introduction}

\section{Convolutional Neural Networks}
Convolutional neural networks are a variation of multilayer perceptrons, a common class of neural networks. Therefore, it makes sense to start there.

\subsection{Multilayer Perceptron}
\subsubsection{Structure}
A classical concrete application of a neural network is the identification of handwritten digits. This application is commonly used as an example thanks to the existence of the MNIST Database (\textit{Modified National Institute of Standards and Technology database}), a database of 70,000 labelled pictures of handwritten digits freely available for machine learning \cite{lecun_mnist_1998}. To simplify the presentation of multilayer perceptrons below, this application will be used to illustrate how the different components of a neural network can work together to achieve this application.

A multilayer perceptron is a class of neural network. As the name implies, it is made up of "neurons" that are connected together. A neuron is simply a real number in the range [0,1] that represents an activation; 0.0 meaning the neuron is not activated and 1.0 meaning the neuron is fully activated. The neurons are arranged in multiple layers, at least three: an input layer, an output layer, and at least one "hidden" layer, each one serving a different role.

Each neuron in the input layer represents a part of the total input provided to the perceptron. For the digits example, the input is a black and white image of a handwritten digit, of format 28*28 pixels. Therefore, the input layer would constitute of 784 neurons, each representing a pixel of the image, and their respective activation being the brightness value of the pixel (0.0 being black, 1.0 being white, and grey shades in-between) \cite{sanderson_but_2017}.

The output layer represents the output of the neural network. As the perceptron performs recognition and classification, each neuron in the output layer is one of the possible "choices" of what could be represented in the image. For the digits example, the output layer would contain ten neurons, one for each digit. The intended output of the neural network is for all but one of the output neurons to have a low activation, and one neuron having a very high activation, the latter being the network's best guess of what digit is represented in the image.

The hidden layers, in-between the input and output layers, serve to improve the performance of the neural network. These layers enable the network to recognize patterns within the image, therefore facilitating the recognition of complex forms. For instance, the digit "8" is made up of two small loops one over the other. In theory, the middle layers could try to recognize these shapes in the image to make a decision whether the digit represented is an eight. In practice, neural networks create their own patterns during the training process, which are often a far cry of what a human would consider logical \cite{sanderson_gradient_2017}.

In a standard multilayer perceptron, with the exception of neurons in the input layer, every neuron from a layer is connected to every neuron from the previous layer. The activation of a neuron affects the activation of all the neurons connected to it ; in other words, the activation of a neuron is a function of the activation of the neurons that preceed it. The connection in-between two layers can be weighted, to increase or decrease how much the activation of a neuron impacts the connection of another neuron.

[MATH FORMULA]

As demonstrated by the formula above, a neural network can be boiled down to fairly simple linear algebra. For this reason, neural networks are often computed by graphical processing units (GPU) instead of regular CPUs, as the formers are optimized for processing linear algebra \cite{salter_cart_2022}.

\subsubsection{Deep learning and backpropagation}


\subsection{Convolutions}

\subsection{Using Image Convolutions in a Neural Network}

\section{Image Analysis with Convolutional Neural Networks}

\section{Favorable Characteristics of CNNs for Image Recognition}

\section{Conclusion}

\clearpage
\begin{flushleft}
\bibliographystyle{plain}
\bibliography{references}
\end{flushleft}

\end{document}