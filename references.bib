
@misc{noauthor_introduction_nodate,
	title = {Introduction to {Image} {Recognition} - {AI} for {Dummies} (1/4) - {Deepomatic} - {Visual} {Automation} {Platform}},
	url = {https://deepomatic.com/introduction-to-computer-vision-and-image-recognition},
	urldate = {2021-12-01},
}

@misc{noauthor_image_nodate,
	title = {Image {Recognition} in 2021: {A} {Comprehensive} {Guide} - viso.ai},
	url = {https://viso.ai/computer-vision/image-recognition/},
	urldate = {2021-12-01},
}

@misc{noauthor_different_nodate,
	title = {Different applications of {Computer} {Vision} - {Deepomatic}},
	url = {https://deepomatic.com/computer-vision-applications},
	urldate = {2021-12-01},
}

@misc{noauthor_image_nodate-1,
	title = {Image {Recognition} : {A} {Complete} {Guide} - {Deepomatic}},
	url = {https://deepomatic.com/what-is-image-recognition},
	urldate = {2021-12-01},
}

@misc{noauthor_object_nodate,
	title = {Object recognition definition and use cases - {Deepomatic}},
	url = {https://deepomatic.com/what-is-object-recognition-and-how-you-can-use-it},
	urldate = {2021-12-01},
}

@misc{noauthor_beginners_nodate,
	title = {A beginner‚Äôs guide to {AI}: {Computer} vision and image recognition},
	url = {https://thenextweb.com/news/a-beginners-guide-to-ai-computer-vision-and-image-recognition},
	urldate = {2021-12-01},
}

@misc{noauthor_computer_nodate,
	title = {Computer {Vision} {Is} {More} {Than} {Just} {Image} {Recognition}},
	url = {https://www.forbes.com/sites/forbestechcouncil/2016/08/12/computer-vision-is-more-than-just-image-recognition/?sh=e4a15846065d},
	urldate = {2021-12-01},
}

@misc{noauthor_gentle_nodate,
	title = {A {Gentle} {Introduction} to {Object} {Recognition} {With} {Deep} {Learning}},
	url = {https://machinelearningmastery.com/object-recognition-with-deep-learning/},
	urldate = {2021-12-01},
}

@misc{noauthor_image_nodate-2,
	title = {Image {Recognition} with {Deep} {Learning} and {Neural} {Networks} {\textbar} {AltexSoft}},
	url = {https://www.altexsoft.com/blog/image-recognition-neural-networks-use-cases/},
	urldate = {2021-12-01},
}

@misc{noauthor_complete_nodate,
	title = {The {Complete} {Beginner}‚Äôs {Guide} to {Deep} {Learning}: {Convolutional} {Neural} {Networks} and {Image} {Classification} {\textbar} by {Anne} {Bonner} {\textbar} {Towards} {Data} {Science}},
	url = {https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb},
	urldate = {2021-12-01},
}

@misc{noauthor_image_nodate-3,
	title = {Image {Recognition} and {Image} {Processing} {Techniques} {\textbar} by {Adoriasoft} {\textbar} {Medium}},
	url = {https://medium.com/@Adoriasoft/image-recognition-and-image-processing-techniques-fe3d35d58919},
	urldate = {2021-12-01},
}

@misc{noauthor_what_nodate,
	title = {What is difference between pattern recognition and object recognition? - {Quora}},
	url = {https://www.quora.com/What-is-difference-between-pattern-recognition-and-object-recognition},
	urldate = {2021-12-01},
}

@misc{noauthor_image_nodate-4,
	title = {Image {Classification} vs {Pattern} {Recognition} vs {Object} {Detection} vs {Object} {Tracking}‚Äì{A} {Primer} ‚Äì {Cloud} {Distilled} {\textasciitilde} {Nithin} {Mohan}},
	url = {https://nitrix-reloaded.com/2018/12/24/image-classification-vs-pattern-recognition-vs-object-detection-vs-object-tracking-a-primer/},
	urldate = {2021-12-01},
}

@misc{sanderson_but_2017,
	title = {But what is a neural network? {\textbar} {Chapter} 1, {Deep} learning},
	shorttitle = {But what is a neural network?},
	url = {https://www.youtube.com/watch?v=aircAruvnKk},
	abstract = {What are the neurons, why are there layers, and what is the math underlying it?
Help fund future projects: https://www.patreon.com/3blue1brown
Written/interactive form of this series: https://www.3blue1brown.com/topics/ne...

Additional funding for this project provided by Amplify Partners

Typo correction: At 14 minutes 45 seconds, the last index on the bias vector is n, when it's supposed to in fact be a k.  Thanks for the sharp eyes that caught that!

For those who want to learn more, I highly recommend the book by Michael Nielsen introducing neural networks and deep learning: https://goo.gl/Zmczdy

There are two neat things about this book.  First, it's available for free, so consider joining me in making a donation Nielsen's way if you get something out of it.  And second, it's centered around walking through some code and data which you can download yourself, and which covers the same example that I introduce in this video.  Yay for active learning!
https://github.com/mnielsen/neural-ne...

I also highly recommend Chris Olah's blog: http://colah.github.io/

For more videos, Welch Labs also has some great series on machine learning: 
https://youtu.be/i8D90DkCLhI
https://youtu.be/bxe2T-V8XRs

For those of you looking to go *even* deeper, check out the text "Deep Learning" by Goodfellow, Bengio, and Courville.  

Also, the publication Distill is just utterly beautiful: https://distill.pub/

Lion photo by Kevin Pluck

-----------------
Timeline: 
0:00 - Introduction example
1:07 - Series preview
2:42 - What are neurons?
3:35 - Introducing layers
5:31 - Why layers?
8:38 - Edge detection example
11:34 - Counting weights and biases
12:30 - How learning relates
13:26 - Notation and linear algebra
15:17 - Recap
16:27 - Some final words
17:03 - ReLU vs Sigmoid

------------------
Animations largely made using manim, a scrappy open source python library.  https://github.com/3b1b/manim

If you want to check it out, I feel compelled to warn you that it's not the most well-documented tool, and has many other quirks you might expect in a library someone wrote with only their own use in mind.

Music by Vincent Rubinetti.
Download the music on Bandcamp:
https://vincerubinetti.bandcamp.com/a...

Stream the music on Spotify:
https://open.spotify.com/album/1dVyjw...

If you want to contribute translated subtitles or to help review those that have already been made by others and need approval, you can click the gear icon in the video and go to subtitles/cc, then "add subtitles/cc".  I really appreciate those who do this, as it helps make the lessons accessible to more people.
------------------

3blue1brown is a channel about animating math, in all senses of the word animate.  And you know the drill with YouTube, if you want to stay posted on new videos, subscribe, and click the bell to receive notifications (if you're into that).

If you are new to this channel and want to see more, a good place to start is this playlist: http://3b1b.co/recommended

Various social media stuffs:
Website: https://www.3blue1brown.com
Twitter: https://twitter.com/3Blue1Brown
Patreon: https://patreon.com/3blue1brown
Facebook: https://www.facebook.com/3blue1brown
Reddit: https://www.reddit.com/r/3Blue1Brown},
	urldate = {2022-01-23},
	author = {Sanderson, Grant},
	month = oct,
	year = {2017},
}

@misc{sanderson_gradient_2017,
	title = {Gradient descent, how neural networks learn {\textbar} {Chapter} 2, {Deep} learning},
	url = {https://www.youtube.com/watch?v=IHZwWFHWa-w},
	abstract = {Enjoy these videos?  Consider sharing one or two.
Help fund future projects: https://www.patreon.com/3blue1brown
Special thanks to these supporters: http://3b1b.co/nn2-thanks
Written/interactive form of this series: https://www.3blue1brown.com/topics/ne...

This video was supported by Amplify Partners.
For any early-stage ML startup founders, Amplify Partners would love to hear from you via 3blue1brown@amplifypartners.com

To learn more, I highly recommend the book by Michael Nielsen
http://neuralnetworksanddeeplearning....
The book walks through the code behind the example in these videos, which you can find here: 
https://github.com/mnielsen/neural-ne...

MNIST database:
http://yann.lecun.com/exdb/mnist/

Also check out Chris Olah's blog: 
http://colah.github.io/
His post on Neural networks and topology is particular beautiful, but honestly all of the stuff there is great.

And if you like that, you'll *love* the publications at distill:
https://distill.pub/

For more videos, Welch Labs also has some great series on machine learning: 
https://youtu.be/i8D90DkCLhI
https://youtu.be/bxe2T-V8XRs

"But I've already voraciously consumed Nielsen's, Olah's and Welch's works", I hear you say.  Well well, look at you then.  That being the case, I might recommend that you continue on with the book "Deep Learning" by Goodfellow, Bengio, and Courville.

Thanks to Lisha Li (@lishali88) for her contributions at the end, and for letting me pick her brain so much about the material.  Here are the articles she referenced at the end:
https://arxiv.org/abs/1611.03530
https://arxiv.org/abs/1706.05394
https://arxiv.org/abs/1412.0233

Music by Vincent Rubinetti: 
https://vincerubinetti.bandcamp.com/a...

-------------------
Video timeline
0:00 - Introduction
0:30 - Recap
1:49 - Using training data
3:01 - Cost functions
6:55 - Gradient descent
11:18 - More on gradient vectors
12:19 - Gradient descent recap
13:01 - Analyzing the network
16:37 - Learning more
17:38 - Lisha Li interview
19:58 - Closing thoughts
------------------

3blue1brown is a channel about animating math, in all senses of the word animate.  And you know the drill with YouTube, if you want to stay posted on new videos, subscribe, and click the bell to receive notifications (if you're into that).

If you are new to this channel and want to see more, a good place to start is this playlist: http://3b1b.co/recommended

Various social media stuffs:
Website: https://www.3blue1brown.com
Twitter: https://twitter.com/3Blue1Brown
Patreon: https://patreon.com/3blue1brown
Facebook: https://www.facebook.com/3blue1brown
Reddit: https://www.reddit.com/r/3Blue1Brown},
	urldate = {2022-01-23},
	author = {Sanderson, Grant},
	month = oct,
	year = {2017},
}

@misc{3blue1brown_what_2017,
	title = {What is backpropagation really doing? {\textbar} {Chapter} 3, {Deep} learning},
	shorttitle = {What is backpropagation really doing?},
	url = {https://www.youtube.com/watch?v=Ilg3gGewQ5U},
	abstract = {What's actually happening to a neural network as it learns?
Help fund future projects: https://www.patreon.com/3blue1brown
An equally valuable form of support is to simply share some of the videos.
Special thanks to these supporters: http://3b1b.co/nn3-thanks
Written/interactive form of this series: https://www.3blue1brown.com/topics/ne...

And by CrowdFlower: http://3b1b.co/crowdflower
Home page: https://www.3blue1brown.com/

The following video is sort of an appendix to this one.  The main goal with the follow-on video is to show the connection between the visual walkthrough here, and the representation of these "nudges" in terms of partial derivatives that you will find when reading about backpropagation in other resources, like Michael Nielsen's book or Chis Olah's blog.

Video timeline:
0:00 - Introduction
0:23 - Recap
3:07 - Intuitive walkthrough example
9:33 - Stochastic gradient descent
12:28 - Final words},
	urldate = {2022-01-23},
	author = {{3Blue1Brown}},
	month = nov,
	year = {2017},
}

@misc{sanderson_backpropagation_2017,
	title = {Backpropagation calculus {\textbar} {Chapter} 4, {Deep} learning},
	url = {https://www.youtube.com/watch?v=tIeHLnjs5U8},
	abstract = {Help fund future projects: https://www.patreon.com/3blue1brown
An equally valuable form of support is to simply share some of the videos.
Special thanks to these supporters: http://3b1b.co/nn3-thanks
Written/interactive form of this series: https://www.3blue1brown.com/topics/ne...

This one is a bit more symbol-heavy, and that's actually the point.  The goal here is to represent in somewhat more formal terms the intuition for how backpropagation works in part 3 of the series, hopefully providing some connection between that video and other texts/code that you come across later.

For more on backpropagation:
http://neuralnetworksanddeeplearning....
https://github.com/mnielsen/neural-ne...
http://colah.github.io/posts/2015-08-...

Music by Vincent Rubinetti:
https://vincerubinetti.bandcamp.com/a...

------------------
Video timeline
0:00 - Introduction
0:38 - The Chain Rule in networks
3:56 - Computing relevant derivatives
4:45 - What do the derivatives mean?
5:39 - Sensitivity to weights/biases
6:42 - Layers with additional neurons
9:13 - Recap
------------------

3blue1brown is a channel about animating math, in all senses of the word animate.  And you know the drill with YouTube, if you want to stay posted on new videos, subscribe, and click the bell to receive notifications (if you're into that): http://3b1b.co/subscribe

If you are new to this channel and want to see more, a good place to start is this playlist: http://3b1b.co/recommended

Various social media stuffs:
Website: https://www.3blue1brown.com
Twitter: https://twitter.com/3Blue1Brown
Patreon: https://patreon.com/3blue1brown
Facebook: https://www.facebook.com/3blue1brown
Reddit: https://www.reddit.com/r/3Blue1Brown},
	urldate = {2022-01-23},
	author = {Sanderson, Grant},
	month = nov,
	year = {2017},
}

@misc{noauthor_multi-layer_nodate,
	title = {Multi-layer perceptron vs deep neural network - {Cross} {Validated}},
	url = {https://stats.stackexchange.com/questions/315402/multi-layer-perceptron-vs-deep-neural-network},
	urldate = {2022-01-23},
}

@misc{noauthor_what_nodate-1,
	title = {what is difference between multilayer perceptron and multilayer neural network? - {Computer} {Science} {Stack} {Exchange}},
	url = {https://cs.stackexchange.com/questions/53521/what-is-difference-between-multilayer-perceptron-and-multilayer-neural-network},
	urldate = {2022-01-23},
}

@misc{noauthor_multilayer_2022,
	title = {Multilayer perceptron},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Multilayer_perceptron&oldid=1065161984},
	abstract = {A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). The term MLP is used ambiguously, sometimes loosely to mean any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation); see ¬ß Terminology. Multilayer perceptrons are sometimes colloquially referred to as "vanilla" neural networks, especially when they have a single hidden layer.An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.},
	language = {en},
	urldate = {2022-01-23},
	journal = {Wikipedia},
	month = jan,
	year = {2022},
	note = {Page Version ID: 1065161984},
	file = {Snapshot:C\:\\Users\\Sebastien\\Zotero\\storage\\SPLTGZ5Y\\Multilayer_perceptron.html:text/html},
}

@misc{noauthor_convolutional_2021,
	title = {Convolutional neural network},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&oldid=1061828869},
	abstract = {In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network, most commonly applied to analyze visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are only equivariant, as opposed to invariant, to translation. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain-computer interfaces, and financial time series.CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The "full connectivity" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns embossed in their filters. Therefore, on a scale of connectivity and complexity, CNNs are on the lower extreme.
Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.},
	language = {en},
	urldate = {2022-01-23},
	journal = {Wikipedia},
	month = dec,
	year = {2021},
	note = {Page Version ID: 1061828869},
	file = {Snapshot:C\:\\Users\\Sebastien\\Zotero\\storage\\FCR456A7\\Convolutional_neural_network.html:text/html},
}

@misc{computerphile_cnn_2016,
	title = {{CNN}: {Convolutional} {Neural} {Networks} {Explained} - {Computerphile}},
	shorttitle = {{CNN}},
	url = {https://www.youtube.com/watch?v=py5byOOHZM8},
	abstract = {Years of work down the drain, the convolutional neural network is a step change in image classification accuracy. Image Analyst Dr Mike Pound explains what it does.

Kernel Convolutions: https://youtu.be/C\_zFhWdM4ic 
Deep Learning: https://youtu.be/l42lr8AlrHk 
Botnets: https://youtu.be/UVFmC178\_Vs 
AI's Game Playing Challenge: https://youtu.be/5oXyibEgJr0
Space Carving: https://youtu.be/cGs90KF4oTc 

http://www.facebook.com/computerphile
https://twitter.com/computer\_phile

This video was filmed and edited by Sean Riley.

Computer Science at the University of Nottingham: http://bit.ly/nottscomputer

Computerphile is a sister project to Brady Haran's Numberphile. More at http://www.bradyharan.com},
	urldate = {2022-01-24},
	author = {{Computerphile}},
	month = may,
	year = {2016},
}

@misc{deep_lizard_convolutional_2017,
	title = {Convolutional {Neural} {Networks} ({CNNs}) explained},
	url = {https://www.youtube.com/watch?v=YRhxdVk_sIs},
	abstract = {CNNs for deep learning
Included in Machine Leaning / Deep Learning for Programmers Playlist:
https://www.youtube.com/playlist?list...

Convolution demo on real data:
https://youtu.be/vJiZqZRkIg8

In this video, we explain the concept of convolutional neural networks, how they're used, and how they work on a technical level. We also discuss the details behind convolutional layers and filters.

fast.ai lesson 4:
http://course17.fast.ai/lessons/lesso...

üïíü¶é VIDEO SECTIONS ü¶éüïí

00:00 Welcome to DEEPLIZARD - Go to deeplizard.com for learning resources
00:30 See convolution demo on real data - Link in the description
08:07 Collective Intelligence and the DEEPLIZARD HIVEMIND

üí•ü¶é DEEPLIZARD COMMUNITY RESOURCES ü¶éüí•

üëã Hey, we're Chris and Mandy, the creators of deeplizard!
üëÄ CHECK OUT OUR VLOG:
üîó https://youtube.com/deeplizardvlog

üëâ Check out the blog post and other resources for this video:
üîó https://deeplizard.com/learn/video/YR...

üíª DOWNLOAD ACCESS TO CODE FILES 
ü§ñ Available for members of the deeplizard hivemind:
üîó https://deeplizard.com/resources

üß† Support collective intelligence, join the deeplizard hivemind:
üîó https://deeplizard.com/hivemind

ü§ú Support collective intelligence, create a quiz question for this video:
üîó https://deeplizard.com/create-quiz-qu...

üöÄ Boost collective intelligence by sharing this video on social media!

‚ù§Ô∏èü¶é Special thanks to the following polymaths of the deeplizard hivemind:
Tammy
Prash
Zach Wimpee

üëÄ Follow deeplizard:
Our vlog: https://youtube.com/deeplizardvlog
Facebook: https://facebook.com/deeplizard
Instagram: https://instagram.com/deeplizard
Twitter: https://twitter.com/deeplizard
Patreon: https://patreon.com/deeplizard
YouTube: https://youtube.com/deeplizard

üéì Deep Learning with deeplizard:
Fundamental Concepts - https://deeplizard.com/learn/video/gZ...
Beginner Code - https://deeplizard.com/learn/video/Rz...
Intermediate Code - https://deeplizard.com/learn/video/v5...
Advanced Deep RL - https://deeplizard.com/learn/video/ny...

üéì Other Courses:
Data Science - https://deeplizard.com/learn/video/d1...
Trading - https://deeplizard.com/learn/video/Zp...

üõí Check out products deeplizard recommends on Amazon:
üîó https://amazon.com/shop/deeplizard

üìï Get a FREE 30-day Audible trial and 2 FREE audio books using deeplizard's link:
üîó https://amzn.to/2yoqWRn

üéµ deeplizard uses music by Kevin MacLeod
üîó https://youtube.com/channel/UCSZXFhRI...
üîó http://incompetech.com/

‚ù§Ô∏è Please use the knowledge gained from deeplizard content for good, not evil.},
	urldate = {2022-01-24},
	author = {{Deep Lizard}},
	month = dec,
	year = {2017},
}

@misc{sanderson_convolutions_2020,
	title = {Convolutions in image processing {\textbar} {Week} 1},
	url = {https://www.youtube.com/watch?v=8rrHTtUzyZA},
	abstract = {The basics of convolutions in the context of image processing.
Course website: 
https://computationalthinking.mit.edu...

Contents: 
0:00 - Introduction
1:12 - Box blur as an average
3:00 - Dealing with the edges
4:31 - Gaussian blur
5:30 - Visualizing gaussian blur
6:04 - Convolution
6:40 - Kernels and the gaussian kernel
7:26 - Looking at the convolution in Julia
8:45 - Julia: `ImageFiltering` package and Kernels
9:08 - Julia: `OffsetArray` with different indices
10:15 - Visualizing a kernel
11:25 - Computational complexity
12:00 - Julia: `prod` function for a product
13:00 - Example of a non-blurring kernel
16:00 - Sharpening edges in an image
17:13 - Edge detection with Sobel filters
21:25 - Relation to polynomial multiplication
25:00 - Convolution in polynomial multiplication
26:08 - Relation to Fourier transforms
28:50 - Fourier transform of an image
31:50 - Convolution via Fourier transform is faster
34:00 - Final thoughts

To learn more about Julia, head to https://julialang.org},
	urldate = {2022-01-25},
	author = {Sanderson, Grant},
	month = sep,
	year = {2020},
}

@misc{alexander_amini_mit_2020,
	title = {{MIT} 6.{S191} (2020): {Convolutional} {Neural} {Networks}},
	shorttitle = {{MIT} 6.{S191} (2020)},
	url = {https://www.youtube.com/watch?v=iaSUYvmCekI},
	abstract = {MIT Introduction to Deep Learning 6.S191: Lecture 3
Convolutional Neural Networks for Computer Vision
Lecturer: Alexander Amini
January 2020

For all lectures, slides, and lab materials: http://introtodeeplearning.com

Lecture Outline
0:00 - Introduction
3:04 - What computers "see"
8:06 - Learning visual features
12:36 - Feature extraction and convolution
19:12 - Convolution neural networks
24:03 - Non-linearity and pooling
28:30 - Code example
29:32 - Applications
32:53 - End-to-end self driving cars
35:55 - Summary

Subscribe to stay up to date with new deep learning lectures at MIT, or follow us @MITDeepLearning on Twitter and Instagram to stay fully-connected!!},
	urldate = {2022-01-25},
	author = {{Alexander Amini}},
	month = feb,
	year = {2020},
}

@misc{lecun_mnist_1998,
	title = {{THE} {MNIST} {DATABASE} of handwritten digits},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2022-03-02},
	journal = {Yann LeCun},
	author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher J.C.},
	year = {1998},
}

@misc{salter_cart_2022,
	address = {Concordia University},
	type = {Lecture},
	title = {{CART} 210 {New} {Media} {Theory}: {Lecture} 10},
	author = {Salter, Christopher},
	month = mar,
	year = {2022},
}

@misc{maruzani_are_2021,
	title = {Are {You} {Unwittingly} {Helping} to {Train} {Google}‚Äôs {AI} {Models}?},
	url = {https://towardsdatascience.com/are-you-unwittingly-helping-to-train-googles-ai-models-f318dea53aee},
	urldate = {2022-03-04},
	journal = {Towards Data Science},
	author = {Maruzani, Rugare},
	month = jan,
	year = {2021},
}

@misc{ibm_cloud_education_what_2020,
	title = {What is {Deep} {Learning}?},
	url = {https://www.ibm.com/cloud/learn/deep-learning},
	urldate = {2022-03-05},
	journal = {IBM},
	author = {{IBM Cloud Education}},
	month = may,
	year = {2020},
	file = {What is Deep Learning? | IBM:C\:\\Users\\Sebastien\\Zotero\\storage\\FG3U3Z7G\\deep-learning.html:text/html},
}

@misc{noauthor_stochastic_nodate,
	title = {Stochastic {Gradient} {Descent} ‚Äî {Clearly} {Explained} !! {\textbar} by {Aishwarya} {V} {Srinivasan} {\textbar} {Towards} {Data} {Science}},
	url = {https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31},
	urldate = {2022-03-06},
}

@misc{noauthor_kernel_2022,
	title = {Kernel (image processing)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Kernel_(image_processing)&oldid=1075390971},
	abstract = {In image processing, a kernel, convolution matrix, or mask is a small matrix used for blurring, sharpening, embossing, edge detection, and more. This is accomplished by doing a convolution between the kernel and an image.},
	language = {en},
	urldate = {2022-03-07},
	journal = {Wikipedia},
	month = mar,
	year = {2022},
	note = {Page Version ID: 1075390971},
	file = {Snapshot:C\:\\Users\\Sebastien\\Zotero\\storage\\WTHFRDRE\\Kernel_(image_processing).html:text/html},
}
